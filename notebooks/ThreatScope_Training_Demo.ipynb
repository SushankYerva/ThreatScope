{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524bb175",
   "metadata": {},
   "source": [
    "\n",
    "# ThreatScope Training Demo\n",
    "\n",
    "This notebook shows how to go from labeled network flow data to a trained intrusion detection model.\n",
    "\n",
    "It covers:\n",
    "1. Loading a prepared dataset of flow features and labels  \n",
    "2. Quick exploratory checks  \n",
    "3. Train/test split  \n",
    "4. Model training (XGBoost)  \n",
    "5. Evaluation (precision, recall, F1, ROC AUC, confusion matrix)  \n",
    "6. Saving `model.joblib`, `feature_order.json`, and `metrics.json` for runtime use\n",
    "\n",
    "This notebook is part of the ThreatScope repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FEATURE_ORDER = [\n",
    "    \"flow_duration_ms\",\n",
    "    \"pkt_rate\",\n",
    "    \"avg_pkt_size\",\n",
    "    \"std_pkt_size\",\n",
    "    \"bytes_total\",\n",
    "    \"syn_count\",\n",
    "    \"fin_count\",\n",
    "    \"psh_count\",\n",
    "    \"entropy_dst_port\",\n",
    "]\n",
    "\n",
    "ARTIFACT_DIR = Path(\"model_artifacts\")\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44937b12",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load dataset\n",
    "\n",
    "Assumptions:\n",
    "- You have already converted raw PCAP traffic into per-flow rows.\n",
    "- Each row has numeric features matching `FEATURE_ORDER`.\n",
    "- There is a `label` column like `benign`, `dos`, `scan`, etc.\n",
    "\n",
    "Edit the path below to point at your CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_CSV = \"datasets/flows_labeled.csv\"  # change this to your file\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# sanity check required columns\n",
    "missing = [c for c in FEATURE_ORDER if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Dataset missing required features: {missing}\")\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"Dataset missing 'label' column\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787eb048",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Class distribution\n",
    "\n",
    "Intrusion datasets are usually imbalanced. For example there may be many benign flows and fewer attack flows like DoS or port scans.\n",
    "\n",
    "We inspect label counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fcfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# bar plot of class distribution using matplotlib (no seaborn)\n",
    "plt.figure()\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Label distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12e01a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Train/test split\n",
    "\n",
    "We stratify on the label so rare attack classes are represented in both train and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[FEATURE_ORDER].astype(float)\n",
    "y = df['label'].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598d939",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Train XGBoost model\n",
    "\n",
    "We use a multiclass softmax/softprob objective. This produces per-class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    n_jobs=-1,\n",
    "    objective='multi:softprob'\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc25538",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Evaluation\n",
    "\n",
    "We compute:\n",
    "- macro precision / recall / F1\n",
    "- ROC AUC macro (one-vs-rest)\n",
    "- confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# classification report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# macro metrics\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "try:\n",
    "    auc_macro = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
    "except Exception:\n",
    "    auc_macro = None\n",
    "\n",
    "print({\n",
    "    'precision_macro': precision_macro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'roc_auc_macro': auc_macro\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061be2ef",
   "metadata": {},
   "source": [
    "\n",
    "### Confusion matrix\n",
    "\n",
    "Rows are true classes. Columns are predicted classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50629d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "print(\"Classes order:\", model.classes_)\n",
    "print(cm)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(model.classes_))\n",
    "plt.xticks(tick_marks, model.classes_, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f753dc3",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Save artifacts for runtime inference\n",
    "\n",
    "Runtime (`ThreatClassifier`) needs:\n",
    "- `model.joblib`\n",
    "- `feature_order.json`\n",
    "- `metrics.json` (optional but good for README badges)\n",
    "\n",
    "These files become part of `model_artifacts/` and get loaded by the FastAPI service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a46429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = {\n",
    "    'precision_macro': float(precision_macro),\n",
    "    'recall_macro': float(recall_macro),\n",
    "    'f1_macro': float(f1_macro),\n",
    "    'roc_auc_macro': float(auc_macro) if auc_macro is not None else None,\n",
    "    'classes': list(model.classes_)\n",
    "}\n",
    "\n",
    "joblib.dump(model, ARTIFACT_DIR / 'model.joblib')\n",
    "\n",
    "with open(ARTIFACT_DIR / 'feature_order.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(FEATURE_ORDER, f, indent=2)\n",
    "\n",
    "with open(ARTIFACT_DIR / 'metrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Artifacts saved in:\", ARTIFACT_DIR.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
